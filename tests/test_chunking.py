import unittest
from unittest.mock import patch, MagicMock
from pathlib import Path
import sys
import os
import typing # For type hints

# Add src directory to Python path to import generate_readme_llm
# This allows the test file to find the module being tested.
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src')))

from generate_readme_llm import parse_and_chunk_repository

class TestParseAndChunkRepositoryChunking(unittest.TestCase):
    """
    Test suite for the `parse_and_chunk_repository` function, focusing on its
    content chunking capabilities.
    """
    # Class attribute to hold the current test instance for mock methods if needed.
    instance: 'TestParseAndChunkRepositoryChunking'

    def setUp(self) -> None:
        """
        Set up common test variables and mock file system structure.
        This method is called before each test function.
        """
        self.repo_path: Path = Path("/fake/repo") # Mock repository root path
        self.display_path: str = "/fake/repo" # Path used for display in logs
        self.extensions: list[str] = [".py", ".txt"] # Default extensions to include
        self.max_chunk_size: int = 256  # Smaller chunk size for chunking tests
        # Store self instance to allow potential mock methods (patched to Path or os)
        # to access test instance's specific attributes like repo_path.
        TestParseAndChunkRepositoryChunking.instance = self

    def _run_chunker_and_collect_chunks(self,
                                        files_structure: dict[str, str],
                                        max_chunk_size: int,
                                        extensions: typing.Optional[list[str]] = None,
                                        include_patterns: typing.Optional[list[str]] = None,
                                        exclude_patterns: typing.Optional[list[str]] = None
                                       ) -> list[str]:
        """
        Helper method to run the `parse_and_chunk_repository` generator with mocked
        file system operations and collect all generated chunks.

        Args:
            files_structure: A dictionary mapping relative file paths (from repo_path)
                             to their content.
            max_chunk_size: The maximum size for each chunk.
            extensions: A list of file extensions to filter by. If None, uses self.extensions.
            include_patterns: A list of glob patterns for including files.
            exclude_patterns: A list of glob patterns for excluding files.

        Returns:
            A list of content chunks (strings) generated by the parser.
        """
        collected_chunks: list[str] = []

        # --- Mocking os.walk ---
        walk_output: list[tuple[str, list[str], list[str]]] = []
        dir_contents: dict[str, tuple[list[str], list[str]]] = {}

        if str(self.repo_path) not in dir_contents:
            dir_contents[str(self.repo_path)] = ([], [])

        for p_rel_str in files_structure.keys():
            p_rel = Path(p_rel_str)
            current_abs_path = self.repo_path / p_rel
            parent_abs_str = str(current_abs_path.parent)
            if parent_abs_str not in dir_contents:
                dir_contents[parent_abs_str] = ([], [])
            if current_abs_path.name not in dir_contents[parent_abs_str][1]:
                 dir_contents[parent_abs_str][1].append(current_abs_path.name)

            ancestor_path = current_abs_path.parent
            while ancestor_path != self.repo_path.parent:
                if str(ancestor_path) not in dir_contents:
                    dir_contents[str(ancestor_path)] = ([], [])
                if ancestor_path != self.repo_path:
                    parent_of_ancestor_str = str(ancestor_path.parent)
                    if parent_of_ancestor_str not in dir_contents:
                         dir_contents[parent_of_ancestor_str] = ([],[])
                    if ancestor_path.name not in dir_contents[parent_of_ancestor_str][0]:
                        dir_contents[parent_of_ancestor_str][0].append(ancestor_path.name)
                if ancestor_path == self.repo_path:
                    break
                ancestor_path = ancestor_path.parent

        for path_str, (dirs, files_in_dir) in sorted(dir_contents.items()):
            walk_output.append((path_str, sorted(list(set(dirs))), sorted(list(set(files_in_dir)))))
        mock_walk = MagicMock(return_value=walk_output)

        # --- Mocking Path.read_text ---
        def mock_read_text_for_path(path_obj: Path, encoding: str ='utf-8') -> str:
            # Use TestParseAndChunkRepositoryChunking.instance for the correct repo_path
            rel_path_str = str(path_obj.relative_to(TestParseAndChunkRepositoryChunking.instance.repo_path))
            return files_structure.get(rel_path_str, "")

        with patch('os.walk', mock_walk), \
             patch.object(Path, 'read_text', mock_read_text_for_path):

            parser_extensions_to_use: list[str] = extensions if extensions is not None else self.extensions
            effective_include_patterns = include_patterns if include_patterns is not None else []
            effective_exclude_patterns = exclude_patterns if exclude_patterns is not None else []

            for chunk in parse_and_chunk_repository(
                str(self.repo_path),
                parser_extensions_to_use,
                self.display_path, # Use display_path from setUp
                max_chunk_size,    # Use passed max_chunk_size
                effective_include_patterns,
                effective_exclude_patterns
            ):
                collected_chunks.append(chunk)
            return collected_chunks

    def test_example_chunking(self) -> None:
        """ Placeholder test for chunking functionality. """
        # Call with empty file structure to ensure the helper runs.
        chunks = self._run_chunker_and_collect_chunks(
            files_structure={},
            max_chunk_size=self.max_chunk_size, # from setUp
            extensions=self.extensions, # from setUp
            include_patterns=[],
            exclude_patterns=[]
        )
        self.assertEqual(chunks, []) # Expect no chunks from an empty repo

    def test_single_small_file(self) -> None:
        """Tests chunking of a single small file that fits in one chunk."""
        file_content = 'print("hello")'
        files_structure: dict[str, str] = {
            "file1.py": file_content
        }

        chunks = self._run_chunker_and_collect_chunks(
            files_structure=files_structure,
            max_chunk_size=self.max_chunk_size, # 256 from setUp
            extensions=self.extensions,       # [".py", ".txt"] from setUp
            include_patterns=[],
            exclude_patterns=[]
        )

        self.assertEqual(len(chunks), 1, "Should produce exactly one chunk.")

        expected_chunk_content = f"# === File: file1.py ===\n{file_content}"
        self.assertEqual(chunks[0], expected_chunk_content, "Chunk content mismatch.")

    def test_single_large_file_truncation(self) -> None:
        """Tests that a single large file is truncated to fit max_chunk_size."""
        original_content = "a" * 500  # Content larger than max_chunk_size
        file_name = "large_file.py"
        files_structure: dict[str, str] = {
            file_name: original_content
        }

        chunks = self._run_chunker_and_collect_chunks(
            files_structure=files_structure,
            max_chunk_size=self.max_chunk_size, # 256 from setUp
            extensions=self.extensions,
            include_patterns=[],
            exclude_patterns=[]
        )

        self.assertEqual(len(chunks), 1, "Should produce exactly one chunk, even if truncated.")

        header = f"# === File: {file_name} ===\n"
        header_size = len(header.encode('utf-8'))

        # Ensure max_content_size is not negative if header is too large
        max_content_size = max(0, self.max_chunk_size - header_size)

        # Expected body is the original content, encoded, truncated by byte length, then decoded
        expected_body_bytes = original_content.encode('utf-8')[:max_content_size]
        expected_body_str = expected_body_bytes.decode('utf-8', 'ignore') # 'ignore' handles potential partial UTF-8 chars

        expected_chunk_content = header + expected_body_str

        self.assertEqual(chunks[0], expected_chunk_content, "Truncated chunk content mismatch.")
        # Also assert that the chunk size is within the limit
        self.assertLessEqual(len(chunks[0].encode('utf-8')), self.max_chunk_size, "Chunk exceeds max_chunk_size.")

    def test_multiple_small_files_single_chunk(self) -> None:
        """Tests that multiple small files are combined into a single chunk if they fit."""
        content1 = "print('file1')"
        file1_name = "file1.py" # Alphabetically first
        header1 = f"# === File: {file1_name} ===\n"

        content2 = "print('file2')"
        file2_name = "file2.py" # Alphabetically second
        header2 = f"# === File: {file2_name} ===\n"

        files_structure: dict[str, str] = {
            # Order in dict might not guarantee processing order,
            # but our mock's sort behavior does.
            file1_name: content1,
            file2_name: content2,
        }

        separator = "\n\n"

        # Calculate total expected size to confirm it's under max_chunk_size
        total_expected_size = len(header1.encode('utf-8')) + len(content1.encode('utf-8')) + \
                              len(separator.encode('utf-8')) + \
                              len(header2.encode('utf-8')) + len(content2.encode('utf-8'))
        self.assertLess(total_expected_size, self.max_chunk_size, "Test setup error: files too large for a single chunk.")

        chunks = self._run_chunker_and_collect_chunks(
            files_structure=files_structure,
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions,
            include_patterns=[],
            exclude_patterns=[]
        )

        self.assertEqual(len(chunks), 1, "Should produce exactly one chunk for multiple small files.")

        # Based on the mocked os.walk, files are sorted by path, so file1.py comes before file2.py
        expected_chunk_content = header1 + content1 + separator + header2 + content2
        self.assertEqual(chunks[0], expected_chunk_content, "Combined chunk content mismatch.")
        self.assertLessEqual(len(chunks[0].encode('utf-8')), self.max_chunk_size, "Chunk exceeds max_chunk_size.")

    def test_multiple_files_cause_splitting(self) -> None:
        """
        Tests that files are split into multiple chunks correctly when their combined
        size exceeds max_chunk_size.
        Scenario:
        - file1 fits in chunk1.
        - file1 + file2 (with headers, separator) exceeds max_chunk_size.
        - file2 starts chunk2.
        - file2 + file3 (with headers, separator) fits in chunk2.
        """
        # max_chunk_size is 256
        separator = "\n\n"
        size_separator = len(separator.encode('utf-8')) # 2 bytes

        # File 1 (f1_alpha.py)
        f1_name = "f1_alpha.py"
        content1 = 'a' * 150
        header1 = f"# === File: {f1_name} ===\n"
        size_header1 = len(header1.encode('utf-8')) # example: 28 bytes for "f1_alpha.py"
        size_content1 = len(content1.encode('utf-8'))
        file1_total_size = size_header1 + size_content1 # 28 + 150 = 178 bytes

        self.assertLessEqual(file1_total_size, self.max_chunk_size, "Test setup: File 1 too large.")

        # File 2 (f2_beta.py)
        f2_name = "f2_beta.py"
        content2 = 'b' * 150
        header2 = f"# === File: {f2_name} ===\n"
        size_header2 = len(header2.encode('utf-8')) # example: 27 bytes for "f2_beta.py"
        size_content2 = len(content2.encode('utf-8'))

        # Check if file1 + file2 would exceed
        size_if_file2_added_to_chunk1 = file1_total_size + size_separator + size_header2 + size_content2
        self.assertGreater(size_if_file2_added_to_chunk1, self.max_chunk_size, "Test setup: File1+File2 should exceed max_chunk_size.")

        # File 3 (f3_gamma.py) - adjusted to fit with File 2 in the second chunk
        f3_name = "f3_gamma.py"
        content3 = 'c' * 49 # Adjusted from 50 to 49
        header3 = f"# === File: {f3_name} ===\n"
        size_header3 = len(header3.encode('utf-8')) # example: 28 bytes for "f3_gamma.py"
        size_content3 = len(content3.encode('utf-8'))

        # Chunk 2 starts with File 2
        chunk2_initial_size = size_header2 + size_content2 # 27 + 150 = 177 bytes
        self.assertLessEqual(chunk2_initial_size, self.max_chunk_size, "Test setup: File 2 itself too large.")

        size_if_file3_added_to_chunk2 = chunk2_initial_size + size_separator + size_header3 + size_content3
        self.assertLessEqual(size_if_file3_added_to_chunk2, self.max_chunk_size,
                             f"Test setup: File2+File3 ({size_if_file3_added_to_chunk2}b) should fit in max_chunk_size ({self.max_chunk_size}b).")

        files_structure: dict[str, str] = {
            f1_name: content1,
            f2_name: content2,
            f3_name: content3,
        }

        chunks = self._run_chunker_and_collect_chunks(
            files_structure=files_structure,
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions,
        )

        self.assertEqual(len(chunks), 2, "Should produce exactly two chunks.")

        # Chunk 1 content
        expected_chunk1_content = header1 + content1
        self.assertEqual(chunks[0], expected_chunk1_content, "Chunk 1 content mismatch.")
        self.assertLessEqual(len(chunks[0].encode('utf-8')), self.max_chunk_size, "Chunk 1 exceeds max_chunk_size.")
        self.assertEqual(len(chunks[0].encode('utf-8')), file1_total_size, "Chunk 1 size mismatch with calculation.")


        # Chunk 2 content
        expected_chunk2_content = header2 + content2 + separator + header3 + content3
        self.assertEqual(chunks[1], expected_chunk2_content, "Chunk 2 content mismatch.")
        self.assertLessEqual(len(chunks[1].encode('utf-8')), self.max_chunk_size, "Chunk 2 exceeds max_chunk_size.")
        self.assertEqual(len(chunks[1].encode('utf-8')), size_if_file3_added_to_chunk2, "Chunk 2 size mismatch with calculation.")

    def test_file_exactly_at_chunk_limit(self) -> None:
        """Tests a single file whose header + content exactly matches max_chunk_size."""
        file_name = "perfect_fit.py"
        header = f"# === File: {file_name} ===\n"

        header_size = len(header.encode('utf-8'))
        # Ensure content_size is not negative.
        self.assertGreaterEqual(self.max_chunk_size, header_size, "Test setup: Header larger than max_chunk_size.")
        content_size = self.max_chunk_size - header_size

        # Use a single-byte character for predictable sizing.
        content_char = 'p'
        content = content_char * content_size

        files_structure: dict[str, str] = {
            file_name: content
        }

        # Pre-assertion to confirm test setup is correct
        expected_total_size = len((header + content).encode('utf-8'))
        self.assertEqual(expected_total_size, self.max_chunk_size,
                         f"Test setup error: Expected size {expected_total_size} does not match max_chunk_size {self.max_chunk_size}.")

        chunks = self._run_chunker_and_collect_chunks(
            files_structure=files_structure,
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions,
        )

        self.assertEqual(len(chunks), 1, "Should produce exactly one chunk.")

        expected_chunk_content = header + content
        self.assertEqual(chunks[0], expected_chunk_content, "Chunk content mismatch.")
        self.assertEqual(len(chunks[0].encode('utf-8')), self.max_chunk_size,
                         "Byte length of the chunk should exactly match max_chunk_size.")

    def test_empty_file_handling(self) -> None:
        """Tests how empty files are handled, both alone and with other files."""

        # Scenario 1: Single empty file
        empty_file_name = "empty_file.py"
        empty_content = ""
        files_structure_1: dict[str, str] = {
            empty_file_name: empty_content
        }

        chunks_1 = self._run_chunker_and_collect_chunks(
            files_structure=files_structure_1,
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions,
        )

        self.assertEqual(len(chunks_1), 1, "Single empty file should produce one chunk.")

        expected_header_empty_file = f"# === File: {empty_file_name} ===\n"
        expected_chunk_1_content = expected_header_empty_file + empty_content
        self.assertEqual(chunks_1[0], expected_chunk_1_content, "Chunk content for single empty file mismatch.")
        self.assertEqual(len(chunks_1[0].encode('utf-8')), len(expected_header_empty_file.encode('utf-8')),
                         "Chunk size for single empty file should be just the header size.")

        # Scenario 2: Empty file combined with a non-empty file
        another_file_name = "another_file.py" # Processed first due to sorting
        another_content = 'print("not empty")'

        # empty_file_name and empty_content are reused from Scenario 1

        files_structure_2: dict[str, str] = {
            another_file_name: another_content,
            empty_file_name: empty_content,
        }

        separator = "\n\n"
        expected_header_another_file = f"# === File: {another_file_name} ===\n"
        # expected_header_empty_file is reused

        expected_chunk_2_content = (
            expected_header_another_file + another_content +
            separator +
            expected_header_empty_file + empty_content
        )

        # Ensure this combination fits in one chunk for the test's purpose
        self.assertLess(len(expected_chunk_2_content.encode('utf-8')), self.max_chunk_size,
                        "Test setup: Combined content for scenario 2 exceeds max_chunk_size.")

        chunks_2 = self._run_chunker_and_collect_chunks(
            files_structure=files_structure_2,
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions,
        )

        self.assertEqual(len(chunks_2), 1, "Combined files (one empty) should produce one chunk.")
        self.assertEqual(chunks_2[0], expected_chunk_2_content, "Combined chunk content (with empty file) mismatch.")

    def test_no_relevant_files_yields_no_chunks(self) -> None:
        """
        Tests scenarios where no chunks should be generated due to lack of relevant files
        or due to filtering.
        """
        # Scenario 1: No files at all in the repository
        chunks_scenario1 = self._run_chunker_and_collect_chunks(
            files_structure={},
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions
        )
        self.assertEqual(len(chunks_scenario1), 0, "Scenario 1: Empty repository should yield no chunks.")

        # Scenario 2: Files present, but none match specified extensions
        files_scenario2 = {
            "file1.md": "# Markdown content",
            "file2.docx": "Word document text"
        }
        # self.extensions is typically ['.py', '.txt'] in setUp
        chunks_scenario2 = self._run_chunker_and_collect_chunks(
            files_structure=files_scenario2,
            max_chunk_size=self.max_chunk_size,
            extensions=self.extensions # Using default [.py, .txt] which don't match .md or .docx
        )
        self.assertEqual(len(chunks_scenario2), 0, "Scenario 2: No files matching extensions should yield no chunks.")

        # Test with specific different extensions to be sure
        chunks_scenario2_specific = self._run_chunker_and_collect_chunks(
            files_structure=files_scenario2,
            max_chunk_size=self.max_chunk_size,
            extensions=['.json', '.yaml']
        )
        self.assertEqual(len(chunks_scenario2_specific), 0, "Scenario 2 (specific): No files matching different extensions should yield no chunks.")


        # Scenario 3: Files present and match extensions, but are excluded by patterns
        files_scenario3 = {
            "important.py": "print('sensitive data')",
            "config/settings.py": "SECRET_KEY = '123'"
        }
        chunks_scenario3_star_py = self._run_chunker_and_collect_chunks(
            files_structure=files_scenario3,
            max_chunk_size=self.max_chunk_size,
            extensions=['.py'], # Ensure .py files are candidates
            exclude_patterns=['*.py'] # Exclude all .py files
        )
        self.assertEqual(len(chunks_scenario3_star_py), 0, "Scenario 3 (*.py): All .py files excluded should yield no chunks.")

        chunks_scenario3_specific_path = self._run_chunker_and_collect_chunks(
            files_structure=files_scenario3,
            max_chunk_size=self.max_chunk_size,
            extensions=['.py'],
            exclude_patterns=['config/*'] # Exclude files in config/ directory
        )
        # This should yield one chunk for important.py, as only config/* is excluded
        self.assertEqual(len(chunks_scenario3_specific_path), 1,
                         "Scenario 3 (specific path): Should yield one chunk for non-excluded important.py.")

        # Now, exclude important.py specifically
        chunks_scenario3_specific_file = self._run_chunker_and_collect_chunks(
            files_structure={"important.py": "print('sensitive data')"}, # only important.py
            max_chunk_size=self.max_chunk_size,
            extensions=['.py'],
            exclude_patterns=['important.py']
        )
        self.assertEqual(len(chunks_scenario3_specific_file), 0,
                         "Scenario 3 (specific file): important.py excluded specifically should yield no chunks.")


if __name__ == '__main__':
    unittest.main()
